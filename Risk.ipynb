{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"24e036c0f2b7c60d7cb7657d2979f348c4a5b08d88c239d1f9cd61f03f76fe4d\"\n",
    "LOCATION_ID = 8833\n",
    "\n",
    "aqi_breakpoints_file = \"aqi_breakpoints.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fetch real-time PM2.5 data from OpenAQ API\n",
    "def fetch_air_quality_data(api_key, location_id):\n",
    "    # Fetch sensors for the given location\n",
    "    location_url = f\"https://api.openaq.org/v3/locations/{location_id}/sensors\"\n",
    "    headers = {\"X-API-Key\": api_key}\n",
    "    response = requests.get(location_url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        sensors_data = response.json()\n",
    "\n",
    "        # Filter sensors to find the one that measures PM2.5\n",
    "        pm25_sensors = []\n",
    "        for sensor in sensors_data[\"results\"]:\n",
    "            if sensor[\"parameter\"][\"name\"] == \"pm25\":\n",
    "                pm25_sensors.append(sensor)\n",
    "\n",
    "        if pm25_sensors:\n",
    "            print(f\"Found {len(pm25_sensors)} PM2.5 sensor(s)\")\n",
    "            for sensor in pm25_sensors:\n",
    "                print(f\"Sensor ID: {sensor['id']}, Sensor Name: {sensor['name']}\")\n",
    "\n",
    "            # Get the daily mean PM2.5 value for the first sensor\n",
    "            sensor_id = pm25_sensors[0][\"id\"]\n",
    "            daily_data_url = f\"https://api.openaq.org/v3/sensors/{sensor_id}/days\"\n",
    "            daily_data_response = requests.get(daily_data_url, headers=headers)\n",
    "\n",
    "            if daily_data_response.status_code == 200:\n",
    "                daily_data = daily_data_response.json()\n",
    "                # Extract the average daily PM2.5 value\n",
    "                if daily_data[\"results\"]:\n",
    "                    daily_mean = daily_data[\"results\"][0][\"value\"]\n",
    "                    print(f\"Daily Mean PM2.5 Value: {daily_mean} µg/m³\")\n",
    "                    return daily_mean\n",
    "                else:\n",
    "                    print(\"No daily data available for PM2.5.\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(\"Error fetching daily mean data\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"No PM2.5 sensors found at this location.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Error fetching sensor data for the location.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dynamic_data_from_dataset(aqi_breakpoints_file, pm25_dataset_file, parameter=\"PM2.5 - Local Conditions\"):\n",
    "    # Load the AQI breakpoints CSV file\n",
    "    aqi_breakpoints = pd.read_csv(aqi_breakpoints_file)\n",
    "    \n",
    "    # Load the PM2.5 dataset\n",
    "    pm25_data = pd.read_csv(pm25_dataset_file)\n",
    "    \n",
    "    # Ensure the files have the necessary columns\n",
    "    required_columns = [\"Parameter\", \"Low Breakpoint\", \"High Breakpoint\", \"AQI Category\"]\n",
    "    if not all(col in aqi_breakpoints.columns for col in required_columns):\n",
    "        raise ValueError(f\"The AQI breakpoints CSV file must contain the following columns: {required_columns}\")\n",
    "    \n",
    "    # Ensure the PM2.5 dataset has the necessary columns\n",
    "    if \"PM2.5 Mean\" not in pm25_data.columns or \"Date\" not in pm25_data.columns:\n",
    "        raise ValueError(\"The PM2.5 dataset must contain 'PM2.5 Mean' and 'Date' columns.\")\n",
    "    \n",
    "    # Filter for the specified parameter\n",
    "    relevant_rows = aqi_breakpoints[aqi_breakpoints[\"Parameter\"] == parameter]\n",
    "    if relevant_rows.empty:\n",
    "        raise ValueError(f\"No data found for the specified parameter: {parameter}\")\n",
    "    \n",
    "    # Categorize each day's PM2.5 mean value\n",
    "    risk_levels = []\n",
    "    for index, row in pm25_data.iterrows():\n",
    "        pm25_value = row[\"PM2.5 Mean\"]\n",
    "        # Find the AQI category based on the PM2.5 value\n",
    "        risk_row = relevant_rows[\n",
    "            (relevant_rows[\"Low Breakpoint\"] <= pm25_value) & \n",
    "            (relevant_rows[\"High Breakpoint\"] >= pm25_value)\n",
    "        ]\n",
    "        if not risk_row.empty:\n",
    "            risk_levels.append(risk_row.iloc[0][\"AQI Category\"])\n",
    "        else:\n",
    "            risk_levels.append(\"Out of Range\")  # Handle values outside of defined breakpoints\n",
    "    \n",
    "    # Add the risk levels to the dataset\n",
    "    pm25_data[\"RiskLevel\"] = risk_levels\n",
    "    \n",
    "    # Return the dataset with the added RiskLevel\n",
    "    return pm25_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train the KNN classifier\n",
    "def train_classifier(data):\n",
    "    # Use the correct column name from your dataset\n",
    "    X = data[[\"PM2.5 Mean\"]]  # Changed from \"PM2.5\" to \"PM2.5 Mean\"\n",
    "    y = data[\"RiskLevel\"]\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train a KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = knn.score(X_test, y_test)\n",
    "    print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "    \n",
    "    return knn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Predict health risk level for real-time PM2.5 data\n",
    "def predict_risk_level(knn_model, pm25_value):\n",
    "    if pm25_value is not None:\n",
    "        prediction = knn_model.predict([[pm25_value]])\n",
    "        print(f\"The Air Quality Index category for PM2.5 = {pm25_value}: {prediction[0]}\")\n",
    "    else:\n",
    "        print(\"No valid PM2.5 data to predict risk level.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 PM2.5 sensor(s)\n",
      "Sensor ID: 25754, Sensor Name: pm25 µg/m³\n",
      "Daily Mean PM2.5 Value: 38.2 µg/m³\n",
      "Model Accuracy: 1.00\n",
      "The Air Quality Index category for PM2.5 = 38.2: UNHEALTHY FOR SENSITIVE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Fetch real-time data\n",
    "real_time_pm25 = fetch_air_quality_data(API_KEY, LOCATION_ID)\n",
    "\n",
    "# Generate synthetic dataset\n",
    "synthetic_data = generate_dynamic_data_from_dataset(aqi_breakpoints_file, \"China_pm25_daily_mean_2020_2024.csv\")\n",
    "\n",
    "# Train the classifier\n",
    "knn_model = train_classifier(synthetic_data)\n",
    "\n",
    "# Predict risk level for real-time data\n",
    "predict_risk_level(knn_model, real_time_pm25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENHANCING Data Quality, and Representativeness: \n",
    "- Currently, the model uses only PM2.5 values. Adding other pollutants (e.g., PM10, NO2, CO)\n",
    "- The synthetic dataset is limited in realism and variability. Fetch historical air quality and health impact data from APIs or other public datasets to train the model on real-world scenarios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING how accurate the Classifier is on other datasets such as AQI in SF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Francisco Test Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the San Francisco dataset for testing\n",
    "sf_dataset_file = \"SF_pm25_daily_mean_2020_2024.csv\"  # Replace with your actual file path\n",
    "sf_data = pd.read_csv(sf_dataset_file)\n",
    "\n",
    "# Ensure the San Francisco dataset has the required columns\n",
    "if \"PM2.5 Mean\" not in sf_data.columns or \"Date\" not in sf_data.columns:\n",
    "    raise ValueError(\"The San Francisco PM2.5 dataset must contain 'PM2.5 Mean' and 'Date' columns.\")\n",
    "\n",
    "# Step 2: Generate the risk levels for the San Francisco dataset using the same function\n",
    "sf_data_with_risk = generate_dynamic_data_from_dataset(aqi_breakpoints_file, sf_dataset_file)\n",
    "\n",
    "# Step 3: Use the trained KNN model to predict the risk levels for the San Francisco dataset\n",
    "X_sf = sf_data_with_risk[[\"PM2.5 Mean\"]]\n",
    "y_sf_true = sf_data_with_risk[\"RiskLevel\"]\n",
    "\n",
    "# Predict the risk levels using the trained KNN model\n",
    "y_sf_pred = knn_model.predict(X_sf)\n",
    "\n",
    "# Evaluate the accuracy of the model on the San Francisco dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "sf_accuracy = accuracy_score(y_sf_true, y_sf_pred)\n",
    "\n",
    "print(f\"San Francisco Test Accuracy: {sf_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
